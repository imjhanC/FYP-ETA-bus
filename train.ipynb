{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "\n",
      "Initial data info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100000 entries, 0 to 99999\n",
      "Data columns (total 39 columns):\n",
      " #   Column                     Non-Null Count   Dtype  \n",
      "---  ------                     --------------   -----  \n",
      " 0   timestamp                  100000 non-null  object \n",
      " 1   bus_id                     100000 non-null  object \n",
      " 2   route_id                   100000 non-null  object \n",
      " 3   stop_sequence              100000 non-null  int64  \n",
      " 4   current_stop_name          100000 non-null  object \n",
      " 5   next_stop_name             100000 non-null  object \n",
      " 6   day_of_week                100000 non-null  int64  \n",
      " 7   is_holiday                 100000 non-null  bool   \n",
      " 8   is_peak_hour               100000 non-null  bool   \n",
      " 9   passenger_count            100000 non-null  float64\n",
      " 10  current_speed              100000 non-null  float64\n",
      " 11  distance_to_next_stop      100000 non-null  float64\n",
      " 12  current_lat                100000 non-null  float64\n",
      " 13  current_lon                100000 non-null  float64\n",
      " 14  eta_minutes                100000 non-null  float64\n",
      " 15  hour                       100000 non-null  int64  \n",
      " 16  timestamp_diff             99999 non-null   float64\n",
      " 17  speed_zscore               100000 non-null  float64\n",
      " 18  eta_zscore                 100000 non-null  float64\n",
      " 19  minute                     100000 non-null  int64  \n",
      " 20  time_of_day                100000 non-null  object \n",
      " 21  bus_id_encoded             100000 non-null  int64  \n",
      " 22  route_id_encoded           100000 non-null  int64  \n",
      " 23  current_stop_name_encoded  100000 non-null  int64  \n",
      " 24  next_stop_name_encoded     100000 non-null  int64  \n",
      " 25  is_holiday_encoded         100000 non-null  int64  \n",
      " 26  is_peak_hour_encoded       100000 non-null  int64  \n",
      " 27  weather_condition_foggy    100000 non-null  bool   \n",
      " 28  weather_condition_rainy    100000 non-null  bool   \n",
      " 29  weather_condition_storm    100000 non-null  bool   \n",
      " 30  weather_condition_sunny    100000 non-null  bool   \n",
      " 31  weather_condition_windy    100000 non-null  bool   \n",
      " 32  traffic_condition_heavy    100000 non-null  bool   \n",
      " 33  traffic_condition_jam      100000 non-null  bool   \n",
      " 34  traffic_condition_light    100000 non-null  bool   \n",
      " 35  traffic_condition_normal   100000 non-null  bool   \n",
      " 36  hour_sin                   100000 non-null  float64\n",
      " 37  hour_cos                   100000 non-null  float64\n",
      " 38  speed_distance_ratio       99502 non-null   float64\n",
      "dtypes: bool(11), float64(12), int64(10), object(6)\n",
      "memory usage: 22.4+ MB\n",
      "None\n",
      "\n",
      "Checking for infinities:\n",
      "timestamp                        0\n",
      "bus_id                           0\n",
      "route_id                         0\n",
      "stop_sequence                    0\n",
      "current_stop_name                0\n",
      "next_stop_name                   0\n",
      "day_of_week                      0\n",
      "is_holiday                       0\n",
      "is_peak_hour                     0\n",
      "passenger_count                  0\n",
      "current_speed                    0\n",
      "distance_to_next_stop            0\n",
      "current_lat                      0\n",
      "current_lon                      0\n",
      "eta_minutes                      0\n",
      "hour                             0\n",
      "timestamp_diff                   0\n",
      "speed_zscore                     0\n",
      "eta_zscore                       0\n",
      "minute                           0\n",
      "time_of_day                      0\n",
      "bus_id_encoded                   0\n",
      "route_id_encoded                 0\n",
      "current_stop_name_encoded        0\n",
      "next_stop_name_encoded           0\n",
      "is_holiday_encoded               0\n",
      "is_peak_hour_encoded             0\n",
      "weather_condition_foggy          0\n",
      "weather_condition_rainy          0\n",
      "weather_condition_storm          0\n",
      "weather_condition_sunny          0\n",
      "weather_condition_windy          0\n",
      "traffic_condition_heavy          0\n",
      "traffic_condition_jam            0\n",
      "traffic_condition_light          0\n",
      "traffic_condition_normal         0\n",
      "hour_sin                         0\n",
      "hour_cos                         0\n",
      "speed_distance_ratio         19603\n",
      "dtype: int64\n",
      "\n",
      "Preparing sequences...\n",
      "\n",
      "Sequence shapes:\n",
      "X shape: (99990, 10, 16)\n",
      "y shape: (99990,)\n",
      "\n",
      "Training LSTM model...\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/100\n",
      "1217/2000 [=================>............] - ETA: 43s - loss: 0.1466 - mae: 0.4271"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Conv1D, MaxPooling1D, Flatten, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def clean_and_prepare_data(df):\n",
    "    \"\"\"\n",
    "    Clean the data by handling infinities and extreme values\n",
    "    \"\"\"\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Replace infinities with NaN\n",
    "    df_clean = df_clean.replace([np.inf, -np.inf], np.nan)\n",
    "    \n",
    "    # Handle numerical columns\n",
    "    numeric_cols = df_clean.select_dtypes(include=[np.number]).columns\n",
    "    for col in numeric_cols:\n",
    "        # Calculate statistics excluding NaN\n",
    "        Q1 = df_clean[col].quantile(0.25)\n",
    "        Q3 = df_clean[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 3 * IQR\n",
    "        upper_bound = Q3 + 3 * IQR\n",
    "        \n",
    "        # Cap extreme values\n",
    "        df_clean[col] = df_clean[col].clip(lower=lower_bound, upper=upper_bound)\n",
    "        \n",
    "        # Fill remaining NaN with median\n",
    "        df_clean[col] = df_clean[col].fillna(df_clean[col].median())\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "def prepare_time_series_data(df, target_col, sequence_length=10):\n",
    "    \"\"\"\n",
    "    Prepare data for time series models with proper cleaning and scaling\n",
    "    \"\"\"\n",
    "    # Clean the data first\n",
    "    df_clean = clean_and_prepare_data(df)\n",
    "    \n",
    "    # Separate continuous and binary features\n",
    "    continuous_features = ['passenger_count', 'current_speed', 'distance_to_next_stop', \n",
    "                         'eta_minutes', 'speed_distance_ratio']\n",
    "    \n",
    "    binary_features = ['weather_condition_foggy', 'weather_condition_rainy', \n",
    "                      'weather_condition_storm', 'weather_condition_sunny',\n",
    "                      'weather_condition_windy', 'traffic_condition_heavy',\n",
    "                      'traffic_condition_jam', 'traffic_condition_light',\n",
    "                      'traffic_condition_normal', 'hour_sin', 'hour_cos']\n",
    "    \n",
    "    # Scale continuous features\n",
    "    scaler = RobustScaler()\n",
    "    df_scaled = df_clean.copy()\n",
    "    df_scaled[continuous_features] = scaler.fit_transform(df_clean[continuous_features])\n",
    "    \n",
    "    # Combine all features\n",
    "    features = continuous_features + binary_features\n",
    "    data = df_scaled[features].astype('float32').values\n",
    "    \n",
    "    sequences = []\n",
    "    targets = []\n",
    "    \n",
    "    for i in range(len(data) - sequence_length):\n",
    "        sequences.append(data[i:(i + sequence_length)])\n",
    "        targets.append(data[i + sequence_length][0])\n",
    "    \n",
    "    return np.array(sequences), np.array(targets), scaler\n",
    "\n",
    "def create_lstm_model(sequence_length, n_features):\n",
    "    \"\"\"\n",
    "    Create LSTM model with improved architecture\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        LSTM(128, activation='relu', input_shape=(sequence_length, n_features), return_sequences=True),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        LSTM(64, activation='relu', return_sequences=True),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        LSTM(32, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        Dense(32, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dense(16, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='huber', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "def create_cnn_model(sequence_length, n_features):\n",
    "    \"\"\"\n",
    "    Create CNN model with improved architecture\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        Conv1D(filters=128, kernel_size=3, activation='relu', padding='same',\n",
    "               input_shape=(sequence_length, n_features)),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        Conv1D(filters=64, kernel_size=3, activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        Conv1D(filters=32, kernel_size=3, activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        Flatten(),\n",
    "        Dense(64, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='huber', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "def train_sarima(data, seasonal_period=24):\n",
    "    \"\"\"\n",
    "    Train SARIMA model with automatic parameter selection\n",
    "    \"\"\"\n",
    "    print(\"Training SARIMA model...\")\n",
    "    best_aic = float('inf')\n",
    "    best_order = None\n",
    "    best_seasonal_order = None\n",
    "    \n",
    "    # Grid search for best parameters\n",
    "    for p in range(2):\n",
    "        for d in range(2):\n",
    "            for q in range(2):\n",
    "                for P in range(2):\n",
    "                    for D in range(2):\n",
    "                        for Q in range(2):\n",
    "                            try:\n",
    "                                model = SARIMAX(data,\n",
    "                                              order=(p,d,q),\n",
    "                                              seasonal_order=(P,D,Q,seasonal_period),\n",
    "                                              enforce_stationarity=False,\n",
    "                                              enforce_invertibility=False)\n",
    "                                results = model.fit(disp=False)\n",
    "                                if results.aic < best_aic:\n",
    "                                    best_aic = results.aic\n",
    "                                    best_order = (p,d,q)\n",
    "                                    best_seasonal_order = (P,D,Q,seasonal_period)\n",
    "                            except:\n",
    "                                continue\n",
    "    \n",
    "    print(f\"Best SARIMA parameters - Order: {best_order}, Seasonal Order: {best_seasonal_order}\")\n",
    "    \n",
    "    final_model = SARIMAX(data,\n",
    "                         order=best_order,\n",
    "                         seasonal_order=best_seasonal_order,\n",
    "                         enforce_stationarity=False,\n",
    "                         enforce_invertibility=False)\n",
    "    return final_model.fit(disp=False)\n",
    "\n",
    "def plot_training_history(history, model_name):\n",
    "    \"\"\"\n",
    "    Plot training history for deep learning models\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(121)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title(f'{model_name} - Training History (Loss)')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    plt.plot(history.history['mae'], label='Training MAE')\n",
    "    plt.plot(history.history['val_mae'], label='Validation MAE')\n",
    "    plt.title(f'{model_name} - Training History (MAE)')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MAE')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_predictions(y_true, y_pred, model_name, scaler=None):\n",
    "    \"\"\"\n",
    "    Plot actual vs predicted values\n",
    "    \"\"\"\n",
    "    if scaler is not None:\n",
    "        # Create dummy array for inverse transform\n",
    "        dummy = np.zeros((len(y_true), scaler.n_features_in_))\n",
    "        dummy[:, 0] = y_true  # Assuming passenger_count is first feature\n",
    "        y_true_orig = scaler.inverse_transform(dummy)[:, 0]\n",
    "        \n",
    "        dummy[:, 0] = y_pred.flatten()\n",
    "        y_pred_orig = scaler.inverse_transform(dummy)[:, 0]\n",
    "    else:\n",
    "        y_true_orig = y_true\n",
    "        y_pred_orig = y_pred\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(y_true_orig, label='Actual', alpha=0.7)\n",
    "    plt.plot(y_pred_orig, label=f'{model_name} Predictions', alpha=0.7)\n",
    "    plt.title(f'{model_name} - Actual vs Predicted')\n",
    "    plt.xlabel('Time Steps')\n",
    "    plt.ylabel('Passenger Count')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def main():\n",
    "    # Load data\n",
    "    print(\"Loading data...\")\n",
    "    df = pd.read_csv('preprocessed_bus_data.csv')\n",
    "    \n",
    "    # Print initial data info\n",
    "    print(\"\\nInitial data info:\")\n",
    "    print(df.info())\n",
    "    print(\"\\nChecking for infinities:\")\n",
    "    print(df.isin([np.inf, -np.inf]).sum())\n",
    "    \n",
    "    # Prepare sequences\n",
    "    print(\"\\nPreparing sequences...\")\n",
    "    sequence_length = 10\n",
    "    X, y, scaler = prepare_time_series_data(df, 'passenger_count', sequence_length)\n",
    "    \n",
    "    print(\"\\nSequence shapes:\")\n",
    "    print(f\"X shape: {X.shape}\")\n",
    "    print(f\"y shape: {y.shape}\")\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "    \n",
    "    # Define callbacks\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    lr_schedule = ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=1e-6\n",
    "    )\n",
    "    \n",
    "    callbacks = [early_stopping, lr_schedule]\n",
    "    \n",
    "    # Train LSTM\n",
    "    print(\"\\nTraining LSTM model...\")\n",
    "    lstm_model = create_lstm_model(sequence_length, X.shape[2])\n",
    "    lstm_history = lstm_model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=100,\n",
    "        batch_size=32,\n",
    "        validation_split=0.2,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Train CNN\n",
    "    print(\"\\nTraining CNN model...\")\n",
    "    cnn_model = create_cnn_model(sequence_length, X.shape[2])\n",
    "    cnn_history = cnn_model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=100,\n",
    "        batch_size=32,\n",
    "        validation_split=0.2,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Train SARIMA\n",
    "    print(\"\\nPreparing SARIMA data...\")\n",
    "    sarima_data = df['passenger_count']\n",
    "    train_size = int(len(sarima_data) * 0.8)\n",
    "    train, test = sarima_data[:train_size], sarima_data[train_size:]\n",
    "    \n",
    "    print(\"Training SARIMA model...\")\n",
    "    sarima_model = train_sarima(train)\n",
    "    \n",
    "    # Make predictions\n",
    "    print(\"\\nMaking predictions...\")\n",
    "    lstm_pred = lstm_model.predict(X_test)\n",
    "    cnn_pred = cnn_model.predict(X_test)\n",
    "    sarima_pred = sarima_model.forecast(len(test))\n",
    "    \n",
    "    # Plot results\n",
    "    print(\"\\nPlotting results...\")\n",
    "    plot_training_history(lstm_history, 'LSTM')\n",
    "    plot_training_history(cnn_history, 'CNN')\n",
    "    \n",
    "    plot_predictions(y_test, lstm_pred, 'LSTM', scaler)\n",
    "    plot_predictions(y_test, cnn_pred, 'CNN', scaler)\n",
    "    plot_predictions(test, sarima_pred, 'SARIMA')\n",
    "    \n",
    "    # Print evaluation metrics\n",
    "    print(\"\\nModel Evaluation:\")\n",
    "    print(\"LSTM - MSE:\", mean_squared_error(y_test, lstm_pred))\n",
    "    print(\"CNN - MSE:\", mean_squared_error(y_test, cnn_pred))\n",
    "    print(\"SARIMA - MSE:\", mean_squared_error(test, sarima_pred))\n",
    "    \n",
    "    return lstm_model, cnn_model, sarima_model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        lstm_model, cnn_model, sarima_model = main()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflowgpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
